{"nbformat":4,"nbformat_minor":0,"metadata":{"environment":{"name":"tf2-gpu.2-1.m46","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"LDAXGB.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"h_DGEBnByZm5","colab_type":"code","colab":{},"outputId":"3dacf2d2-09c8-44f1-9545-8ab3c8e5a2d3"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","import datetime\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","from nltk import SnowballStemmer\n","from nltk.stem import LancasterStemmer, WordNetLemmatizer\n","from nltk.corpus import wordnet\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","from nltk.tokenize import WhitespaceTokenizer\n","import inflect\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","from xgboost.sklearn import XGBRegressor\n","from sklearn.model_selection import GridSearchCV\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/jupyter/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /home/jupyter/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"byvE28KzrJUX","colab_type":"text"},"source":["## Generate Dataset"]},{"cell_type":"code","metadata":{"id":"qVX7nFR-yZm-","colab_type":"code","colab":{}},"source":["class Dataset(object):\n","    def __init__(self, corpusSource,scoreSource):\n","        self._corpusSource = corpusSource\n","        self._scoreSource=scoreSource\n","        self._genX()\n","        \n","    def _readData(self, filePath1,filePath2):\n","        score=pd.read_csv(filePath1)\n","        score.drop(columns=['Unnamed: 0'],inplace=True)\n","        score.rename(columns={\"Name\":\"company\"},inplace=True)\n","        corpus=pd.read_csv(filePath2,names=['company','text'])\n","        corpus1=corpus.dropna()\n","        self.score=score\n","        self.corpus=corpus1\n","    \n","    def get_wordnet_pos(self,pos_tag): # POS Tagging\n","        if pos_tag.startswith('J'):\n","            return wordnet.ADJ\n","        elif pos_tag.startswith('V'):\n","            return wordnet.VERB\n","        elif pos_tag.startswith('N'):\n","            return wordnet.NOUN\n","        elif pos_tag.startswith('R'):\n","            return wordnet.ADV\n","        else:\n","            return wordnet.NOUN\n","\n","    def remove_punctuation(self,words):\n","        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n","        new_words = []\n","        for word in words:\n","            new_word = re.sub(r'[^\\w\\s]', '', word)\n","            if new_word != '':\n","                new_words.append(new_word)\n","        return new_words\n","\n","    def remove_special(self,words):\n","        \"\"\"Remove special signs like &*\"\"\"\n","        new_words = []\n","        for word in words:\n","            new_word = re.sub(r'[-,$()#+&*]', '', word)\n","            if new_word != '':\n","                new_words.append(new_word)\n","        return new_words\n","\n","    def replace_numbers(self,words):\n","        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n","        p = inflect.engine()\n","        new_words = []\n","        for word in words:\n","            try:\n","                if word.isdigit():\n","                    new_word = p.number_to_words(word)\n","                    new_words.append(new_word)\n","                else:\n","                    new_words.append(word)\n","            except:\n","                continue\n","        return new_words\n","\n","    def remove_stopwords(self,words):\n","        \"\"\"Remove stop words from list of tokenized words\"\"\"  \n","        stopwords = nltk.corpus.stopwords.words('english')\n","        myStopWords = []\n","        stopwords.extend(myStopWords)\n","        new_words = []\n","        for word in words:\n","            if word not in stopwords:\n","                new_words.append(word)\n","        return new_words\n","\n","\n","    def normalize_lemmatize(self,words):\n","        words = self.remove_special(words)\n","        words = self.remove_punctuation(words)\n","        words = self.replace_numbers(words)\n","        words = self.remove_stopwords(words)\n","        pos_tags = pos_tag(words) # POS Tagging\n","        # Lemmatize words based on tags\n","        words = [WordNetLemmatizer().lemmatize(t[0], self.get_wordnet_pos(t[1])) for t in pos_tags]\n","        # remove words with only one letter\n","        words = [t for t in words if len(t) > 1]\n","        #words = lemmatize_verbs(words)\n","        #words = stem_words(words)\n","        return words\n","    \n","    def _genX(self):\n","        self._readData(self._scoreSource,self._corpusSource)\n","        x_feature=self.corpus.values[:,1]\n","        for i in range(len(x_feature)):\n","            words = nltk.word_tokenize(x_feature[i])\n","            x_feature[i]=self.normalize_lemmatize(words)\n","        sx_feature=np.array(list(map(lambda x: \" \".join(x), x_feature)))\n","        self.x_feature=sx_feature\n","        \n","        \n","data = Dataset(\"real whole corpus.csv\",\"real BB company.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e5lrxYturoAo","colab_type":"text"},"source":["## Build Model"]},{"cell_type":"code","metadata":{"id":"pEpkUqxPyZnA","colab_type":"code","colab":{}},"source":["class LDA_XGB:\n","    def __init__(self,data):\n","        self.corpus=data.corpus\n","        self.score=data.score\n","        self.x_feature=data.x_feature\n","        cv = CountVectorizer(max_df=0.95, min_df=2,\n","                        stop_words='english')\n","        self.df = cv.fit_transform(self.x_feature)\n","    def find_LDA(self,max_topics):\n","        n_topics = range(2, max_topics+1)\n","        perplexityLst = [1.0]*len(n_topics)\n","        lda_models = []\n","        for idx, n_topic in enumerate(n_topics):\n","            lda = LatentDirichletAllocation(n_components=n_topic,\n","                                            max_iter=100,\n","                                            learning_method='batch',\n","                                            evaluate_every=200,\n","        #                                    perp_tol=0.1, #default                                       \n","        #                                    doc_topic_prior=1/n_topic, #default\n","        #                                    topic_word_prior=1/n_topic, #default\n","                                            verbose=0)\n","            lda.fit(self.df)\n","            perplexityLst[idx] = lda.perplexity(self.df)\n","            lda_models.append(lda)\n","            print (\"# of Topic: %d, \" % n_topics[idx])\n","            print (\"Perplexity Score %0.3f\" % perplexityLst[idx])\n","        fig = plt.figure()\n","        ax = fig.add_subplot(1,1,1)\n","        ax.plot(n_topics, perplexityLst)\n","        ax.set_xlabel(\"# of topics\")\n","        ax.set_ylabel(\"Approximate Perplexity\")\n","        plt.grid(True)\n","        plt.show()\n","        best_index = perplexityLst.index(min(perplexityLst))\n","        best_n_topic = n_topics[best_index]\n","        best_model = lda_models[best_index]\n","        print (\"Best # of Topic: \", best_n_topic)\n","        return best_model,best_n_topic\n","    \n","    def gen_LDAfeature(self,model):\n","        self.LDA_features = model.fit_transform(self.df)\n","        \n","    def gen_XY(self,best_n_topic):\n","        df_new=pd.DataFrame(np.hstack((self.corpus.values,self.LDA_features)))\n","        df_new.rename(columns={0:'company'},inplace=True)\n","        data=pd.merge(df_new, score, on=['company'], how='inner').drop_duplicates('company').reset_index(drop=True)\n","        X=data.iloc[:,2:2+best_n_topic].values\n","        y=data.iloc[:,-1]\n","        return X,y\n","    \n","    def select_model(self,x,y,nthread,objective,learning_rate,max_depth,\n","                 min_child_weight,subsample,colsample_bytree,n_estimators):\n","        parameters = {'nthread':nthread, #when use hyperthread, xgboost may become slower\n","              'objective':objective,\n","              'learning_rate': learning_rate, #so called `eta` value\n","              'max_depth': max_depth,\n","              'min_child_weight': min_child_weight,\n","              'subsample': subsample,\n","              'colsample_bytree': colsample_bytree,\n","              'n_estimators': n_estimators}\n","        \n","        xgb1 = XGBRegressor()\n","        xgb_grid = GridSearchCV(xgb1,\n","                        parameters,\n","                        cv = 2,\n","                        n_jobs = 5,\n","                        verbose=True)\n","        xgb_grid.fit(x,y)\n","        print(\"best score:\",xgb_grid.best_score_)\n","        print(\"best params:\",xgb_grid.best_params_)\n","        self.best_model=xgb_grid.best_estimator_\n","        \n","    \n","    def train(self,x,y):\n","        self.best_model.fit(x,y)\n","    \n","    def predict(self,x):\n","        return self.best_model.predict(x)\n","            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdynyMCUyZnD","colab_type":"code","colab":{}},"source":["model=LDA_XGB(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"svnZvXdcyZnF","colab_type":"code","colab":{},"outputId":"d8ffd8a1-8752-4d6e-c950-9c03c1a5b159"},"source":["best_LDA,best_n_topic=model.find_LDA(20)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# of Topic: 2, \n","Perplexity Score 1602.016\n","# of Topic: 3, \n","Perplexity Score 1545.424\n","# of Topic: 4, \n","Perplexity Score 1493.590\n","# of Topic: 5, \n","Perplexity Score 1478.273\n","# of Topic: 6, \n","Perplexity Score 1452.974\n","# of Topic: 7, \n","Perplexity Score 1439.899\n","# of Topic: 8, \n","Perplexity Score 1434.327\n","# of Topic: 9, \n","Perplexity Score 1415.307\n","# of Topic: 10, \n","Perplexity Score 1414.075\n","# of Topic: 11, \n","Perplexity Score 1402.609\n","# of Topic: 12, \n","Perplexity Score 1396.207\n","# of Topic: 13, \n","Perplexity Score 1380.062\n","# of Topic: 14, \n","Perplexity Score 1375.141\n","# of Topic: 15, \n","Perplexity Score 1364.635\n","# of Topic: 16, \n","Perplexity Score 1374.033\n","# of Topic: 17, \n","Perplexity Score 1357.873\n","# of Topic: 18, \n","Perplexity Score 1349.202\n","# of Topic: 19, \n","Perplexity Score 1353.298\n","# of Topic: 20, \n","Perplexity Score 1345.372\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3s5gsQnLyZnI","colab_type":"code","colab":{}},"source":["model.gen_LDAfeature(best_LDA)\n","X,y=gen_XY(best_n_topic)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3kZvvNlByZnL","colab_type":"code","colab":{}},"source":["trainX,trainY,testX,testY=train_test_split(X,y,test_size=0.2,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAhJ03pKyZnP","colab_type":"code","colab":{},"outputId":"7d966236-018f-4510-fcfd-d88c4aebd19c"},"source":["## grid search and train\n","model.select_model(trainX,trainY,nthread=[4],objective=['reg:squarederror'],learning_rate=[.03, 0.05, .07],max_depth=[5, 6, 7],\n","                     min_child_weight=[4],subsample=[0.7],colsample_bytree=[0.7],n_estimators=[500])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n","[Parallel(n_jobs=5)]: Done  18 out of  18 | elapsed:   20.3s finished\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=2, error_score=nan,\n","             estimator=XGBRegressor(base_score=None, booster=None,\n","                                    colsample_bylevel=None,\n","                                    colsample_bynode=None,\n","                                    colsample_bytree=None, gamma=None,\n","                                    gpu_id=None, importance_type='gain',\n","                                    interaction_constraints=None,\n","                                    learning_rate=None, max_delta_step=None,\n","                                    max_depth=None, min_child_weight=None,\n","                                    missing=nan, monotone_constraints=None,\n","                                    n_estima...\n","                                    validate_parameters=None, verbosity=None),\n","             iid='deprecated', n_jobs=5,\n","             param_grid={'colsample_bytree': [0.7],\n","                         'learning_rate': [0.03, 0.05, 0.07],\n","                         'max_depth': [5, 6, 7], 'min_child_weight': [4],\n","                         'n_estimators': [500], 'nthread': [4],\n","                         'objective': ['reg:squarederror'],\n","                         'subsample': [0.7]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring=None, verbose=True)"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"hchbgXkVyZnS","colab_type":"code","colab":{},"outputId":"f8632887-524f-4faf-a14a-4e27dd92b031"},"source":["model.train(trainX,trainY)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n","             importance_type='gain', interaction_constraints='',\n","             learning_rate=0.03, max_delta_step=0, max_depth=7,\n","             min_child_weight=4, missing=nan, monotone_constraints='()',\n","             n_estimators=500, n_jobs=4, nthread=4, num_parallel_tree=1,\n","             objective='reg:squarederror', random_state=0, reg_alpha=0,\n","             reg_lambda=1, scale_pos_weight=1, subsample=0.7,\n","             tree_method='exact', validate_parameters=1, verbosity=None)"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"W225aDngyZnU","colab_type":"code","colab":{},"outputId":"02604c28-9258-4b17-e985-7541a1908adf"},"source":["#training MSE\n","MSE = np.mean((model.predict(trainX) - trainY)**2)\n","print(MSE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.8207264822547455\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZIL6ywssyZnX","colab_type":"code","colab":{},"outputId":"0ac70b90-51b2-40a6-8bd4-71bdfa5c5791"},"source":["# Testing MSE:\n","MSE1 = np.mean((model.predict(testX)- testY)**2)\n","print(MSE1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["159.43091710030242\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LMANjR5ByZnb","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}